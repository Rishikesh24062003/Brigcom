# BridgComm_Model_ISL.ipynb

# Introduction
"""
This notebook demonstrates the implementation of BridgComm: A Video Intercom for the Deaf using the Indian Sign Language (ISL) dataset.
The project aims to bridge communication gaps between hearing and non-hearing impaired individuals
by recognizing ISL gestures and converting them into voice messages and vice versa.
"""

# Setup
## Import necessary libraries
import cv2
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import numpy as np
import pyttsx3
import speech_recognition as sr

## Set up the Raspberry Pico, camera, microphone, and other peripherals
# (Assume Raspberry Pico setup via Thonny or another IDE for embedded development)

# Camera setup
camera = cv2.VideoCapture(0)  # Adjust the index depending on your camera

# Microphone setup
recognizer = sr.Recognizer()

# Speaker setup
engine = pyttsx3.init()

# Sign Language Recognition using ISL Dataset
"""
This section covers loading the ISL dataset, training a model for gesture recognition, processing
video input from the webcam, and displaying recognized gestures.
"""
## Load the ISL dataset using Keras
datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)

train_generator = datagen.flow_from_directory(
    'path_to_isl_dataset',  # Replace with the path to your ISL dataset
    target_size=(64, 64),
    batch_size=32,
    class_mode='categorical',
    subset='training'
)

validation_generator = datagen.flow_from_directory(
    'path_to_isl_dataset',
    target_size=(64, 64),
    batch_size=32,
    class_mode='categorical',
    subset='validation'
)

## Define and train the model
model = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2, 2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.fit(
    train_generator,
    validation_data=validation_generator,
    epochs=10  # Adjust the number of epochs based on your dataset size
)

## Save the trained model
model.save('isl_gesture_recognition_model.h5')

## Load pre-trained models for gesture recognition
model = load_model('isl_gesture_recognition_model.h5')

## Process video input from the webcam
def recognize_gesture(frame):
    # Preprocess the frame
    processed_frame = preprocess_frame(frame)
    # Predict the gesture
    prediction = model.predict(np.array([processed_frame]))
    return np.argmax(prediction), np.max(prediction)

def preprocess_frame(frame):
    # Resize and normalize the frame
    frame_resized = cv2.resize(frame, (64, 64))
    frame_normalized = frame_resized / 255.0
    return frame_normalized

## Display recognized gestures
while True:
    ret, frame = camera.read()
    gesture_id, confidence = recognize_gesture(frame)
    cv2.putText(frame, f"Gesture: {gesture_id}, Confidence: {confidence:.2f}", 
                (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)
    cv2.imshow('ISL Gesture Recognition', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

camera.release()
cv2.destroyAllWindows()

# Text-to-Speech (TTS)
"""
This section converts recognized ISL gestures into text and then uses a TTS engine
to generate corresponding voice messages.
"""
def gesture_to_text(gesture_id):
    # Map the gesture ID to text based on ISL
    gesture_map = {
        0: "Hello",
        1: "Yes",
        2: "No",
        # Add more mappings based on your dataset
    }
    return gesture_map.get(gesture_id, "Unknown gesture")

def text_to_speech(text):
    engine.say(text)
    engine.runAndWait()

# Example usage
gesture_id = 0  # Example gesture ID
text = gesture_to_text(gesture_id)
text_to_speech(text)

# Speech-to-Sign Language (STS)
"""
This section captures and processes voice input, converts it into text, and then
generates corresponding ISL gestures for display.
"""
def speech_to_text():
    with sr.Microphone() as source:
        print("Listening...")
        audio = recognizer.listen(source)
        try:
            text = recognizer.recognize_google(audio)
            print(f"Recognized: {text}")
            return text
        except sr.UnknownValueError:
            print("Could not understand the audio")
            return None

def text_to_gesture(text):
    # Map text to a corresponding gesture based on ISL
    text_map = {
        "hello": 0,
        "yes": 1,
        "no": 2,
        # Add more mappings based on your dataset
    }
    return text_map.get(text.lower(), -1)

# Example usage
text = speech_to_text()
if text:
    gesture_id = text_to_gesture(text)
    if gesture_id != -1:
        print(f"Displaying gesture: {gesture_id}")
        # Code to display the gesture on the screen
    else:
        print("Gesture not recognized")

# Integration
"""
Integrate all components into a cohesive system, where ISL gesture recognition,
TTS, and STS work together to facilitate communication.
"""
def main():
    while True:
        # Capture gesture
        ret, frame = camera.read()
        gesture_id, confidence = recognize_gesture(frame)
        if confidence > 0.8:  # Threshold for gesture recognition
            text = gesture_to_text(gesture_id)
            text_to_speech(text)

        # Capture voice input and convert to gesture
        text = speech_to_text()
        if text:
            gesture_id = text_to_gesture(text)
            if gesture_id != -1:
                print(f"Displaying gesture: {gesture_id}")
                # Code to display the gesture on the screen

        # Break loop with 'q'
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    camera.release()
    cv2.destroyAllWindows()

if __name__ == "__main__":
    main()

# Testing and Validation
"""
Test the system with sample ISL inputs and validate the accuracy and performance of
gesture recognition, TTS, and STS modules.
"""
# Example test case for ISL gesture recognition
test_frame = np.zeros((64, 64, 3))  # Replace with actual test frame
gesture_id, confidence = recognize_gesture(test_frame)
assert gesture_id == expected_gesture_id, f"Expected {expected_gesture_id}, got {gesture_id}"
print("Test passed!")

